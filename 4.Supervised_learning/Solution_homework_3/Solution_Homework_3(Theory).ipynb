{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a) L1 regularization shrinks parameters to zero and provides sparse solutions. \n",
    "Sparse solution means that the majority of the input features have zero weights and only a few have non zero weights.\n",
    "L2 regularization forces the weights to be small but does not make them zero,\n",
    "which means it provides a non sparse solution.\n",
    "\n",
    "If using gradient descent, you will iteratively make the weights change in the opposite direction of the gradient with a step size ùúÇ multiplied with the gradient.\n",
    "This means that a more steep gradient will make us take a larger step, while a more flat gradient will make us take a smaller step.\n",
    " ùêø1-regularization will move any weight towards 0 with the same step size, regardless the weight's value. In contrast,\n",
    "ùêø2 gradient is linearly decreasing towards 0 as the weight goes towards 0. Therefore, ùêø2-regularization will also move any weight towards 0, but it will take smaller and smaller steps as a weight approaches 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b)\n",
    "L1 regularization act as feature selector.\n",
    "It assigns insignificant input features with zero weight and useful features with a non zero weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a) Approaches to avoid Overfitting of Decision Tree\n",
    "\n",
    "\n",
    " * Pre-pruning - stops growing the tree earlier, before it perfectly classifies the training set. <br>\n",
    " * Post-pruning - allows the tree to perfectly classify the training set, and then post prune the tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b)\n",
    "\n",
    "* Bagging :  When Decision Trees are combined in parallel, variance across datasets is erduced. This leads to a more robust model. \n",
    "\n",
    "* Boosting(Adaboost) : When decision Trees are sequentially combined, it addresses the bias.\n",
    "Fit consecutive trees at every step, so the data are learned sequentially. Wrong  predictions in a previous iteration are improved with each next iteration. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\n",
    "Standardization must be done, before fitting a regularized model. Standardization of feature make the value in the data to have zero-mean and unit-variance.\n",
    "All features are evenly scaled and  weights of all features are penalized equally.\n",
    "\n",
    "Without the standardization of the features, the results would not be equivariant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
